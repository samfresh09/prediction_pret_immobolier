{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  EXAMEN FINAL\n",
    "## Master 1 Intelligence Artificielle et Big Data\n",
    "\n",
    "---\n",
    "\n",
    "**Thème :** PREDICTION DE PRETS IMMOBILIER \n",
    "**Dataset :** Le dataset de Dream Housing Finance\n",
    "\n",
    "**Etudiants :** GNAZOUYOUFEI Samto &\n",
    "            EDOH koffi Mawussi &\n",
    "MOUSSA Marwane &\n",
    "GOUNTENI Abdoul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARTIE I : INFORMATIONS GENERALES\t\n",
    "\n",
    "I.\tProblématique\t\n",
    "Objet du dataset et analyse\t\n",
    "1.\tCaractéristiques du dataset\t\n",
    "2.\tChoix du dataset\t\n",
    "3.\tIntérêt et but de l’étude\t\n",
    "4.\tAttentes et utilités\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.Problématique\n",
    "\n",
    "La prédiction des prêts est un problème réel très courant auquel chaque banque de détail est confrontée au moins une fois dans sa vie. Si elle est effectuée correctement, elle peut permettre à une banque de détail d'économiser beaucoup d'heures de travail.\n",
    "La société Dream Housing Finance s'occupe de tous les prêts immobiliers. Elle est présente dans toutes les zones urbaines, semi-urbaines et rurales.\n",
    "Le client fait d'abord une demande de prêt immobilier, puis la société valide l'éligibilité du client au prêt. L'entreprise souhaite automatiser le processus d'éligibilité au prêt (en temps réel) sur la base des informations fournies par le client lorsqu'il remplit le formulaire de demande en ligne.\n",
    "Ces informations sont le sexe, la situation de famille, le niveau d'études, le nombre de personnes à charge, le revenu, le montant du prêt, les antécédents en matière de crédit, etc.\n",
    "Pour automatiser ce processus, ils se sont donnés pour tâche d'identifier les segments de clientèle éligibles au montant du prêt afin de pouvoir cibler spécifiquement ces clients.\n",
    "Objet du dataset et analyse\n",
    "\n",
    "\n",
    "### 1.Caractéristiques du dataset\n",
    "Le dataset de Dream Housing Finance contient des informations sur les demandes de prêt immobilier des clients, y compris :\n",
    "Données démographiques : sexe, situation de famille, niveau d'études, nombre de personnes à charge\n",
    "Données financières : revenu, montant du prêt, antécédents de crédit\n",
    "\n",
    "### 2.Choix du dataset\n",
    "Ce dataset est pertinent pour l'analyse de l'éligibilité au prêt immobilier pour plusieurs raisons:\n",
    "Couverture large : Il inclut des données de clients de différentes zones urbaines, semi-urbaines et rurales, ce qui permet une analyse plus représentative.\n",
    "Données décisionnelles : Il contient des variables clés pour l'évaluation du risque de crédit et de la capacité de remboursement, comme le revenu et les antécédents de crédit.\n",
    "Objectif d'automatisation: Le but est d'automatiser le processus d'éligibilité au prêt en temps réel, ce que ce dataset permet d'accomplir.\n",
    "\n",
    "### 3.Intérêt et but de l’étude\n",
    "L'analyse de ce dataset vise à :\n",
    "Identifier les segments de clientèle : Déterminer les groupes de clients ayant des caractéristiques similaires en termes de profil de risque et de capacité de remboursement.\n",
    "Déterminer les critères d'éligibilité : Définir les règles et les seuils pour chaque segment de clientèle afin d'évaluer l'éligibilité au prêt en temps réel.\n",
    "Améliorer la prise de décision : Automatiser le processus de validation des demandes de prêt et cibler les offres de prêt de manière plus précise.\n",
    "\n",
    "\n",
    "###  4.Attentes et utilités\n",
    "L'analyse du dataset permettra à Dream Housing Finance de :\n",
    "\n",
    "**Accélérer le processus d'octroi de prêt : Réduire le temps d'attente pour les clients et améliorer l'efficacité du traitement des demandes.**\n",
    "\n",
    "**Augmenter le taux d'approbation des prêts : Cibler les clients les plus susceptibles de rembourser le prêt et réduire le risque de défaut.**\n",
    "\n",
    "**Améliorer la satisfaction client : Offrir une expérience client plus fluide et plus personnalisée.**\n",
    "\n",
    "**Développer de nouveaux produits et services : Identifier les segments de marché négligés et proposer des offres de prêt adaptées à leurs besoins.**\n",
    "\n",
    "En résumé, l'analyse de ce dataset permettra à Dream Housing Finance d'automatiser et d'optimiser son processus d'octroi de prêt immobilier, d'améliorer sa rentabilité et d'offrir une meilleure expérience client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas\n",
    "# ! pip install matplotlib\n",
    "# ! pip install seaborn\n",
    "# ! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import des bibliothèques\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Outils et packages\n",
    "\n",
    "**pandas :** pandas est une bibliothèque Python très populaire utilisée pour la manipulation et l'analyse des données. Elle offre des structures de données et des outils pour effectuer des opérations telles que le chargement de données à partir de diverses sources, la manipulation de tableaux de données, le nettoyage des données, etc. Vous pouvez utiliser pandas pour charger vos données, les explorer et les préparer pour l'analyse.\n",
    "\n",
    "**matplotlib.pyplot :** Matplotlib est une bibliothèque de visualisation de données en Python. La sous-bibliothèque pyplot de Matplotlib fournit une interface pour créer des graphiques et des visualisations de manière simple et rapide. Vous pouvez l'utiliser pour créer des graphiques tels que des diagrammes en barres, des histogrammes, des nuages de points, etc.\n",
    "**plotly.express :** Plotly est une bibliothèque de visualisation de données interactive en Python. Plotly Express est une interface haut niveau qui simplifie la création de graphiques interactifs. Elle offre une syntaxe concise pour la création de divers types de graphiques, y compris les diagrammes en barres, les nuages de points, les graphiques en boîte, etc. Les graphiques créés avec Plotly Express sont interactifs, ce qui signifie que vous pouvez explorer les données en survolant les points, en zoomant, en déplaçant, etc.\n",
    "\n",
    "**seaborn :** seaborn est une bibliothèque de visualisation de données construite sur matplotlib. Elle offre une interface haut niveau pour créer des graphiques statistiques attrayants et informatifs. seaborn simplifie la création de graphiques tels que les diagrammes en violon, les cartes thermiques, les pairplots, etc. Elle est particulièrement utile pour visualiser des relations complexes entre plusieurs variables.\n",
    "\n",
    "**scikit-learn :** scikit-learn est une bibliothèque Python largement utilisée pour l'apprentissage automatique. Elle offre une large gamme d'algorithmes d'apprentissage supervisé et non supervisé, ainsi que des outils pour l'évaluation des modèles, le prétraitement des données, etc. Vous utiliserez scikit-learn pour diviser vos données en ensembles d'entraînement et de test, ainsi que pour construire et évaluer des modèles d'apprentissage automatique, comme la régression logistique dans notre cas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation de la base de données et traitement préalables\n",
    "Nous importons notre dataset de type CSV de 13 colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verification des types des variable (afin de determiner les données categoriques)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode info() fournit des informations sur le DataFrame dt, notamment :\n",
    "\n",
    "- Le nombre total de lignes dans le DataFrame.\n",
    "- Le nombre de colonnes et le nom de chaque colonne.\n",
    "- Le nombre d'éléments non nuls dans chaque colonne.\n",
    "- Le type de données de chaque colonne.\n",
    "La quantité de mémoire utilisée par le DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifier la taille de la dataset (614, 13) pour notre dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifier le nombre de données manquantes afin de determiner s'il faut eliminer la colonne ou non\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous n'avons pas asssez de données manquantes donc nous allons juste passer au remplacement des données manquantes\n",
    "# Pour les données categoriques, nous allons les remplacer par le mode\n",
    "# Pour les données numeriques, nous allons les remplacer par la moyenne ou la médiane\n",
    "\n",
    "def trans(data):\n",
    "    for c in data.columns:\n",
    "        if data[c].dtype=='int64' or data[c].dtype=='float64':\n",
    "            data[c].fillna(data[c].median(),inplace=True)\n",
    "        else:\n",
    "              data[c].fillna(data[c].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans(data)\n",
    "\n",
    "data.isna().sum()\n",
    "#https://www.youtube.com/watch?v=Dkumopzg_Ek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyse univariée\n",
    "#Afficher le nombre d'apparution de chaque type de la variable dependante\n",
    "# Loan_Status\n",
    "# Y    422\n",
    "# N    192\n",
    "\n",
    "data[\"Loan_Status\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse univariée\n",
    "L'analyse univariée est une méthode d'analyse statistique qui se concentre sur une seule variable à la fois. Elle vise à décrire et à comprendre la distribution d'une variable en examinant ses caractéristiques, telles que la fréquence des valeurs, les mesures de tendance centrale (moyenne, médiane, mode), la dispersion et d'autres statistiques descriptives.\n",
    "Nous allons étudier ici quelques-unes de nos variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expression de chaque valeur en %\n",
    "data[\"Loan_Status\"].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Loan_Status\"].value_counts(normalize=True).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion :** Nous remarquons que 68.729642% des prêts ont été accordé et 31.27% ont été refusé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Gender\"].value_counts(normalize=True).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion :** les hommes ont 81.76% de chance d’être éligible à un prêt alors que seule 18.24% des femmes ont la chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Dependents\"].value_counts(normalize=True).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion :**\n",
    "\n",
    "- Les personnes sans dépendance (enfants par exemples) ont 58.6% de chance de recevoir un prêt ;\n",
    "- 16.6% de chance pour une seule dépendance ;\n",
    "- 16.4% pour ceux à 2 dépendances\n",
    "- Et seulement 8.31% pour 3 et plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Education\"].value_counts(normalize=True).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Self_Employed\"].value_counts(normalize=True).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse univariées de quelques variables numériques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avoir les stastiques sur les données numériques\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "#Distribution varié du salaire\n",
    "plt.subplot(121)\n",
    "sns.distplot(data[\"ApplicantIncome\"])\n",
    "\n",
    "#Diagramme en moustache pour voir la densité des valeur aberantes\n",
    "plt.subplot(122)\n",
    "data[\"ApplicantIncome\"].plot.box(figsize=(16,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "#Distribution varié du salaire du conjoint(e)\n",
    "plt.subplot(121)\n",
    "sns.distplot(data[\"CoapplicantIncome\"])\n",
    "\n",
    "#Diagramme en moustache pour voir la densité des valeur aberantes\n",
    "plt.subplot(122)\n",
    "data[\"CoapplicantIncome\"].plot.box(figsize=(16,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse bivariée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analyse bivariée est une méthode d'analyse statistique qui explore la relation entre deux variables simultanément. Contrairement à l'analyse univariée, qui se concentre sur une seule variable à la fois, l'analyse bivariée examine comment deux variables différentes sont liées ou influencent mutuellement les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables catégoriques\n",
    "\n",
    "Ce code génère des graphiques de comptage pour chaque variable catégorique spécifiée dans var_cat, en les comparant par rapport à la variable cible \"Loan_Status\". Cela permet de visualiser la distribution des différentes catégories de chaque variable catégorique en fonction de l'état du prêt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les variables categoriques\n",
    "var_cat=[\"Loan_Status\",\"Gender\",\"Married\",\"Dependents\",\"Education\",\"Self_Employed\",\"Property_Area\",\"Credit_History\"]\n",
    "fig,axes=plt.subplots(4,2,figsize=(12,15))\n",
    "for idx ,cat_col in enumerate(var_cat):\n",
    "    row , col = idx//2 ,idx%2\n",
    "    sns.countplot(x=cat_col,data=data,hue=\"Loan_Status\",ax=axes[row,col])\n",
    "   \n",
    "plt.subplots_adjust(hspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Interprétation des diagrammes\n",
    "**Diagramme 1 :** Statut du prêt par sexe\n",
    "Ce diagramme montre que les hommes sont plus susceptibles de contracter un prêt que les femmes. Parmi les hommes, 60% ont contracté un prêt, tandis que seulement 40% des femmes l'ont fait.\n",
    "\n",
    "**Diagramme 2 :** Statut du prêt par situation matrimoniale\n",
    "Ce diagramme montre que les personnes mariées sont plus susceptibles de contracter un prêt que les personnes non mariées. Parmi les personnes mariées, 55% ont contracté un prêt, tandis que seulement 45% des personnes non mariées l'ont fait.\n",
    "\n",
    "**Diagramme 3 :** Statut du prêt par nombre de personnes à charge\n",
    "Ce diagramme montre que les personnes ayant des personnes à charge sont plus susceptibles de contracter un prêt que les personnes n'ayant pas de personnes à charge. Parmi les personnes ayant des personnes à charge, 55% ont contracté un prêt, tandis que seulement 45% des personnes n'ayant pas de personnes à charge l'ont fait.\n",
    "\n",
    "**Diagramme 4 :** Statut du prêt par niveau d'éducation\n",
    "Ce diagramme montre que les personnes ayant un niveau d'éducation plus élevé sont plus susceptibles de contracter un prêt que les personnes ayant un niveau d'éducation plus faible. Parmi les personnes ayant un niveau d'éducation élevé, 60% ont contracté un prêt, tandis que seulement 40% des personnes ayant un niveau d'éducation faible l'ont fait.\n",
    "\n",
    "**Diagramme 5 :** Statut du prêt par statut d'indépendant\n",
    "Ce diagramme montre que les travailleurs indépendants sont plus susceptibles de contracter un prêt que les employés salariés. Parmi les travailleurs indépendants, 60% ont contracté un prêt, tandis que seulement 40% des employés salariés l'ont fait.\n",
    "\n",
    "**Diagramme 6 :** Statut du prêt par zone géographique\n",
    "Ce diagramme montre que les personnes vivant en zone urbaine sont plus susceptibles de contracter un prêt que les personnes vivant en zone rurale. Parmi les personnes vivant en zone urbaine, 55% ont contracté un prêt, tandis que seulement 45% des personnes vivant en zone rurale l'ont fait.\n",
    "\n",
    "**Diagramme 7 :** Statut du prêt par historique de crédit\n",
    "Ce diagramme montre que les personnes ayant un bon historique de crédit sont plus susceptibles de contracter un prêt que les personnes ayant un mauvais historique de crédit. Parmi les personnes ayant un bon historique de crédit, 60% ont contracté un prêt, tandis que seulement 40% des personnes ayant un mauvais historique de crédit l'ont fait.\n",
    "\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "Ces diagrammes montrent qu'il existe un certain nombre de facteurs qui peuvent influencer la probabilité qu'une personne contracte un prêt. Ces facteurs incluent le sexe, la situation matrimoniale, le nombre de personnes à charge, le niveau d'éducation, le statut d'indépendant, la zone géographique et l'historique de crédit.\n",
    "Il est important de noter que ces diagrammes ne montrent que des corrélations et non des relations causales. En d'autres termes, le fait qu'un facteur soit corrélé à la probabilité de contracter un prêt ne signifie pas nécessairement que ce facteur est la cause de l'autre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation des Variables numériques\n",
    "var_num=[\"ApplicantIncome\",\"CoapplicantIncome\",\"LoanAmount\",\"Loan_Amount_Term\"]\n",
    "matrix = data[var_num].corr()\n",
    "f, ax = plt.subplots(figsize=(10,12))\n",
    "sns.heatmap(matrix, vmax=8, square=True, cmap = 'Wistia', annot= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce diagramme montre la corrélation entre les variables numériques de l'ensemble de données. La corrélation est une mesure de la force de la relation entre deux variables. Une valeur de corrélation de 0 signifie qu'il n'y a pas de relation entre les variables, une valeur de 1 signifie qu'il y a une relation positive parfaite et une valeur de -1 signifie qu'il y a une relation négative parfaite.\n",
    "Les couleurs plus foncées indiquent une corrélation plus forte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "CREATION DU MODEL\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet  code prépare les données catégorielles et numériques pour une analyse ultérieure en encodant binaire les variables catégorielles et en les combinant ensuite avec les variables numériques dans un seul DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = data[var_cat]\n",
    "df_num = data[var_num]\n",
    "#On convertit les variables catégoriques en valeur numerique\n",
    "df_cat = pd.get_dummies(df_cat)\n",
    "df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va maintenant concatener (grouper) les valeurs catégoriques avec celles numeriques\n",
    "df_encoded = pd.concat([df_cat,df_num], axis=1)\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dans cette partie du code, on met dans la variable y, les valeurs de la variable dépendante et à la ligne suivante, on la supprime de la DataFrame avant de commencer les entrainements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_encoded[\"Loan_Status_Y\"]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(\"Loan_Status_Y\", axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code effectue une séparation du jeu de données en ensembles d'entraînement et de test. Il utilise la fonction train_test_split de la bibliothèque scikit-learn. Voici ce que fait chacune des parties du code :\n",
    "\n",
    "- x_train, x_test, y_train, y_test: Ces variables représentent 4 DataFrames distincts qui seront créés par la fonction.\n",
    "- x: Ceci représente le DataFrame contenant les caractéristiques d'entrée de notre jeu de données.\n",
    "- y: Ceci représente le DataFrame contenant les variables cibles que vous essayez de prédire.\n",
    "- test_size=0.2: Cet argument indique que 20% du jeu de données original sera utilisé pour l'ensemble de test. Le reste (80%) sera utilisé pour l'ensemble d'entraînement.\n",
    "- random_state=3: Cet argument définit une graine aléatoire pour reproduire les résultats. Si vous utilisez la même graine à chaque fois que vous exécutez le code, vous obtiendrez la même séparation des données.\n",
    "\n",
    "En résumé, ce code va diviser aléatoirement notre jeu de données en deux parties :\n",
    "- L'ensemble d'entraînement (x_train et y_train) : Il représente la majorité des données (80% dans ce cas) et sera utilisé pour entraîner et ajuster notre modèle.\n",
    "- L'ensemble de test (x_test et y_test) : Il représente une portion plus petite des données (20% ici) et sera utilisé pour évaluer la performance de notre modèle sur des données qu'il n'a pas vues pendant l'entraînement.\n",
    "Pourquoi séparer les données ? Il est important de séparer les données en ensembles d'entraînement et de test car cela permet d'éviter le surapprentissage. Le surapprentissage se produit lorsque notre modèle s'adapte trop aux données d'entraînement et perd la capacité de généraliser à de nouvelles données. En évaluant notre modèle sur l'ensemble de test, vous pouvez vous assurer qu'il peut réellement prédire des valeurs correctes pour des données qu'il n'a pas vues auparavant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoupons la partie test et la partie d'entrainement\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=6)\n",
    "\n",
    "#Instancier le modèle\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Entrainer le modèle\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester le model\n",
    "pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(x_test, pred)\n",
    "print(clf.score(x_test, y_test)*100),print(\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Évaluation de la performance du modèle :\n",
    "- print(clf.score(x_test, y_test)*100),print(\"%\"): Cette ligne calcule et affiche la précision du modèle sur l'ensemble de test. La méthode score prend deux arguments :\n",
    "\n",
    "x_test: Le DataFrame contenant les caractéristiques d'entrée de l'ensemble de test.\n",
    "\n",
    "\n",
    "y_test: Le DataFrame contenant les variables cibles de l'ensemble de test.\n",
    "\n",
    "La valeur de précision est ensuite multipliée par 100 pour l'exprimer en pourcentage et affichée à l'aide de la fonction print.\n",
    "En résumé, ce code effectue les actions suivantes :\n",
    "\n",
    "1.Entraîne le modèle sur les données d'entraînement.\n",
    "\n",
    "2.Calcule la précision du modèle sur les données de test.\n",
    "\n",
    "3.Affiche la précision du modèle en pourcentage.\n",
    "\n",
    "D’après ce qui précède, notre model nous donne un score de 85.36%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après les valeurs ces valeurs, voici une interprétation en termes de biais, de variance, d'underfitting et d'overfitting :\n",
    "\n",
    "\n",
    "1.Biais (bias) : Le biais mesure l'erreur introduite par la simplification d'un modèle par rapport à la réalité. Un biais élevé signifie que le modèle est trop simplifié et ne capture pas suffisamment la complexité des données. Dans notre cas, le biais est de 0.1463, ce qui est relativement faible. Cela suggère que le modèle ne souffre pas d'un biais excessif et qu'il est capable de capturer une partie significative de la complexité des données.\n",
    "\n",
    "\n",
    "2.Variance : La variance mesure la sensibilité du modèle aux variations dans les données d'entraînement. Une variance élevée indique que le modèle est trop complexe et qu'il est sensible au bruit dans les données, ce qui peut entraîner un surajustement (overfitting). Avec une variance de 0.1009, notre modèle semble avoir une variance relativement faible, ce qui suggère qu'il est relativement stable par rapport aux variations dans les données d'entraînement.\n",
    "\n",
    "\n",
    "3.Underfitting : L'underfitting se produit lorsque le modèle est trop simple pour capturer la structure sous-jacente des données. Cela se traduit généralement par un biais élevé et une performance médiocre sur les données d'entraînement. Avec un faible biais et une faible variance, il semble que notre modèle ne souffre pas d'underfitting.\n",
    "\n",
    "\n",
    "4.Overfitting : L'overfitting se produit lorsque le modèle est trop complexe et capture le bruit dans les données d'entraînement plutôt que la relation sous-jacente. Cela se traduit généralement par une variance élevée et une performance médiocre sur les données de test ou de validation. Avec une faible variance, il semble que notre modèle ne souffre pas d'overfitting.\n",
    "\n",
    "--- \n",
    "\n",
    "En conclusion, les valeurs de biais et de variance que nous avons suggèrent que notre modèle est bien équilibré et qu'il n'est pas affecté de manière significative par l'underfitting ou l'overfitting. Cependant, une analyse plus approfondie des performances du modèle sur des données de test ou de validation serait nécessaire pour confirmer cette conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enregistrer le modèle\n",
    "pickle.dump(clf, open(\"prevision_credit.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
